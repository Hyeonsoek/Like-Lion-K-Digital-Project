{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 폐기한 크롤링 코드\n",
    "\n",
    ">+ 본 파일은 폐기했던 크롤링 코드에 대해서 다룹니다.\n",
    ">+ 파일 설명 : selium으로 모든 유튜브 페이지를 크롤링하여 정보를 가져오는 코드입니다. <br/>\n",
    "소요 예상 시간은 100개 채널 데이터 기준으로 약 2~3시간이며,<br/> \n",
    "저희 조에서 가져올려고 했던 5개의 카테고리 각 100개의 채널을 가져오는데는 10시간 이상이 들어가서 폐기했습니다.<br/>\n",
    ">+ Appendix라서 매우 대충 설명할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 모듈 import\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 영상 url 한개를 가져와서 스크롤 다운하고 정보 끌어오는 식으로 짰음\n",
    "# 유튜브 페이지는 일단 브라우저에서 보이지 않으면 크롤링이 안되서 스크롤 하는 식으로 짰음\n",
    "# 그런데 스크롤 다운 속도가 너무 빠르면 댓글창이 로딩이 안되서 나누기 3으로 천천히 내려서 긁어옴\n",
    "def video_info(video_urls):\n",
    "    driver.get(video_urls)\n",
    "    time.sleep(3)\n",
    "\n",
    "    while True:\n",
    "        lastHeight = driver.execute_script(\"return document.documentElement.scrollHeight/3\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight/3);\")\n",
    "        time.sleep(1)\n",
    "        newHeight = driver.execute_script(\"return document.documentElement.scrollHeight/3\")\n",
    "        if newHeight == lastHeight:\n",
    "             break\n",
    "\n",
    "    video_source = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    trans = lambda x : 0 if '없음' in x else int(x[3:-1].replace(',',''))\n",
    "\n",
    "    view_count = video_source.find('span',{'class':'view-count'}).get_text()\n",
    "    view_count = trans(view_count)\n",
    "\n",
    "    opinion = video_source.find_all('yt-formatted-string',{'class':'ytd-toggle-button-renderer'})\n",
    "\n",
    "    like_count = trans(opinion[0]['aria-label'])\n",
    "    dlike_count = trans(opinion[1]['aria-label'])\n",
    "    \n",
    "    comment_count = 1\n",
    "    for find_stop in video_source.find_all('span',{'class':'style-scope yt-formatted-string'}):\n",
    "        if '댓글이 사용 중지되었습니다.' in find_stop.get_text():\n",
    "            comment_count = 0\n",
    "    \n",
    "    if comment_count:\n",
    "        comment_count = video_source.find('yt-formatted-string',\n",
    "                                    {'class':'count-text style-scope ytd-comments-header-renderer'}).find_all('span',{'dir':'auto'})[1].get_text()\n",
    "        comment_count = int(comment_count)\n",
    "\n",
    "    video_info = [ view_count, like_count, dlike_count, comment_count ]\n",
    "    return video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채널 정보를 받아서 그 채널 url로 들어가고 \n",
    "# 동영상 탭에서 최근 30개 영상의 url을 video_info 함수에 파라미터로 넘겨주고 동영상 데이터를 받아옴\n",
    "# return은 영상 내 정보를 모두 더한값 한 개의 series\n",
    "def channel_quest(channel):\n",
    "    channel_url = 'https://www.youtube.com/' + channel[9:]\n",
    "    driver.get(channel_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    tab_content = driver.find_elements_by_class_name('tab-content')\n",
    "\n",
    "    for tab in tab_content:\n",
    "        if (tab.text == '동영상') | (tab.text == 'Videos'):\n",
    "            tab.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    web = driver.page_source\n",
    "    source = BeautifulSoup(web, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        videos = source.find_all('a',{'id':'video-title'})\n",
    "        video_urls = []\n",
    "        videos_df = []\n",
    "        for video in videos:\n",
    "            video_url = 'https://www.youtube.com' + video['href']\n",
    "            videos_df.append(video_info(video_url))\n",
    "            time.sleep(3)\n",
    "    \n",
    "        videos_df = pd.DataFrame(videos_df, columns=['view','like','dislike','comments'])\n",
    "        return videos_df.sum()\n",
    "    except:\n",
    "        return pd.Series([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selium 켜고 타겟 url 보내주고 거기서 250개의 유튜버 정보를 크롤링해옴\n",
    "# 그걸 파일에 저장함\n",
    "driver = webdriver.Chrome(executable_path='chromedriver.exe')\n",
    "\n",
    "influence = \"https://kr.noxinfluencer.com/youtube-channel-rank/top-250-kr-film%20%26%20animation-youtuber-sorted-by-subs-weekly\"\n",
    "\n",
    "driver.get(influence)\n",
    "time.sleep(5)\n",
    "\n",
    "while True:\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if newHeight == lastHeight:\n",
    "         break\n",
    "\n",
    "web = driver.page_source\n",
    "source = BeautifulSoup(web, 'html.parser')\n",
    "\n",
    "channel_href = []\n",
    "channel_names = []\n",
    "channel_subscribe = []\n",
    "\n",
    "for channel in source.find_all('a',{'class','star-avatar'}):\n",
    "    if channel['href'] != '':\n",
    "        channel_href.append(channel['href'])\n",
    "        \n",
    "for channel_name in source.find_all('span',{'class':'name kol-name'}):\n",
    "    channel_names.append(channel_name['title'])\n",
    "    \n",
    "for channel_subs in source.find_all('td',{'class':'text followerNum with-num'}):\n",
    "    channel_subscribe.append( channel_subs.find('span',{'class':'num'}).get_text().strip() )\n",
    "    \n",
    "channels_info = pd.DataFrame( [x for x in zip(channel_names,channel_subscribe,channel_href) ], columns=['name','subs','url'])\n",
    "channels_info.to_excel ('channel_info.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채널 정보 저장된 파일 불러와서 함수 실행시켜서 모든 채널의 정보(영상 30개 정도의 합)을 가져옴\n",
    "ani_df = pd.DataFrame(columns=['view','like','dislike','comments'])\n",
    "channels_info = pd.read_excel('channel_info.xlsx',encoding='utf-8')\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='chromedriver.exe')\n",
    "for idx in range(10):\n",
    "    print('now : ' + channels_info.loc[idx]['url'])\n",
    "    channel_return = channel_quest(channels_info.loc[idx]['url'])\n",
    "    ani_df.loc[idx] = channel_return\n",
    "\n",
    "ani_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selinum 정보\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
